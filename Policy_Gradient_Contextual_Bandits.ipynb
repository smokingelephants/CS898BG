{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smokingelephants/CS898BG/blob/main/Policy_Gradient_Contextual_Bandits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UegzQxNOG1O_"
      },
      "source": [
        "## Solving Contextual Bandit problem using Policy Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYZYycZvG1PC"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9Lp4x6Z6G1PD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ce49c6-dfb2-4d33-bf33-1e0b0110ada6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#import tf.contrib.slim as slim\n",
        "import tf_slim as slim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxspWr07G1PG"
      },
      "source": [
        "The problem : Contextual Bandits\n",
        "    \n",
        "*  The environment consists of several states(bandits) and are independent of each other. \n",
        "*  Given a state(bandit) the agent understands the environment and tries to make best possible action(pull arm) which results in better rewards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v4i-Tl2SG1PH"
      },
      "outputs": [],
      "source": [
        "class contextual_bandit():\n",
        "    def __init__(self):\n",
        "        self.state = 0\n",
        "        self.bandits = np.array([[0.2, 0.0, -0.0, -5], [0.1, -5, 1, 0.25], [-5, 5, 5, 5]])\n",
        "        self.num_bandits = self.bandits.shape[0]\n",
        "        self.num_actions = self.bandits.shape[1]\n",
        "        \n",
        "    def getBandit(self):\n",
        "        #Choosing a random bandit \n",
        "        self.state = np.random.randint(0, self.num_bandits)\n",
        "        return self.state\n",
        "    \n",
        "    def pullArm(self, action):\n",
        "        #\n",
        "        bandit = self.bandits[self.state, action]\n",
        "        result = np.random.randn(1)\n",
        "        if(result > bandit):\n",
        "            return +2 #1\n",
        "        else:\n",
        "            return -2 #-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJSwn7aMG1PJ"
      },
      "source": [
        "Let's define the agent..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KicdkRXxG1PL"
      },
      "outputs": [],
      "source": [
        "class agent():\n",
        "    def __init__(self, lr, s_size, a_size):\n",
        "        #defining feed-forward network which takes states as input and produce action as output.\n",
        "        self.state_in = tf.placeholder(shape=[1], dtype=tf.int32)\n",
        "        state_in_OH = slim.one_hot_encoding(self.state_in, s_size)\n",
        "        output = slim.fully_connected(state_in_OH, a_size, biases_initializer=None, activation_fn=tf.nn.softmax, weights_initializer=tf.ones_initializer())\n",
        "        self.output = tf.reshape(output, [-1])\n",
        "        self.choosen_action = tf.argmax(self.output, 0)\n",
        "        \n",
        "        #We train the neural network by feeding the reward and choosen action to compute loss and therefore update the network\n",
        "        self.reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
        "        self.action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
        "        self.responsible_weight = tf.slice(self.output, self.action_holder, [1])\n",
        "        self.loss = -(tf.log(self.responsible_weight) * self.reward_holder)\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "        self.update = optimizer.minimize(self.loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQHMDsViG1PM"
      },
      "source": [
        "Training the agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aqJ2aq9G1PN",
        "outputId": "d1b8e115-9302-4053-e84d-a3e92c38c692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.25 0.25 0.25 0.25]\n",
            "The average reward for each of the 3 bandits: [ 0.  -0.5  0. ]\n",
            "[0.23068893 0.31239697 0.2284508  0.22846325]\n",
            "The average reward for each of the 3 bandits: [71.  78.5 74. ]\n",
            "[0.21143505 0.3735229  0.20711924 0.20792273]\n",
            "The average reward for each of the 3 bandits: [149.5 153.  135. ]\n",
            "[0.19015871 0.4364815  0.18653719 0.18682252]\n",
            "The average reward for each of the 3 bandits: [228.  227.  203.5]\n",
            "[0.17071564 0.49341536 0.16733402 0.16853501]\n",
            "The average reward for each of the 3 bandits: [301.5 305.5 279.5]\n",
            "[0.53778005 0.15406731 0.15406096 0.15409167]\n",
            "The average reward for each of the 3 bandits: [381.5 376.5 338.5]\n",
            "[0.5870815  0.13736369 0.13817228 0.13738255]\n",
            "The average reward for each of the 3 bandits: [461.  452.  417.5]\n",
            "[0.12419049 0.1222432  0.123542   0.6300243 ]\n",
            "The average reward for each of the 3 bandits: [543.5 516.  492. ]\n",
            "[0.11675537 0.656816   0.11267662 0.11375202]\n",
            "The average reward for each of the 3 bandits: [627.5 582.  562. ]\n",
            "[0.10522301 0.69148296 0.10108168 0.10221235]\n",
            "The average reward for each of the 3 bandits: [703.5 670.  621. ]\n",
            "[0.0921046  0.09183608 0.09290893 0.7231504 ]\n",
            "The average reward for each of the 3 bandits: [791.  739.  692.5]\n",
            "[0.75096273 0.08323221 0.08328361 0.08252148]\n",
            "The average reward for each of the 3 bandits: [868.5 812.5 766.5]\n",
            "[0.7723189  0.07594026 0.07628799 0.07545276]\n",
            "The average reward for each of the 3 bandits: [949.  891.5 828. ]\n",
            "[0.7905336  0.06972738 0.07017653 0.06956247]\n",
            "The average reward for each of the 3 bandits: [1021.5  978.   899. ]\n",
            "[0.06980344 0.79656154 0.06596927 0.06766567]\n",
            "The average reward for each of the 3 bandits: [1094.  1041.5  986. ]\n",
            "[0.06523389 0.8106312  0.06111057 0.06302434]\n",
            "The average reward for each of the 3 bandits: [1173.  1116.5 1055. ]\n",
            "[0.05875957 0.05840578 0.05967668 0.8231579 ]\n",
            "The average reward for each of the 3 bandits: [1242.5 1186.  1129. ]\n",
            "[0.05754879 0.8338448  0.05338609 0.05522028]\n",
            "The average reward for each of the 3 bandits: [1321.  1256.  1207.5]\n",
            "[0.86297196 0.04599641 0.04531195 0.04571965]\n",
            "The average reward for each of the 3 bandits: [1387.5 1334.5 1285.5]\n",
            "[0.05041817 0.04964112 0.05076538 0.84917533]\n",
            "The average reward for each of the 3 bandits: [1462.  1414.5 1358. ]\n",
            "[0.04755931 0.86164176 0.04427016 0.04652874]\n",
            "The average reward for each of the 3 bandits: [1537.5 1493.5 1429.5]\n",
            "[0.04451048 0.8706364  0.04138299 0.04347014]\n",
            "The average reward for each of the 3 bandits: [1618.5 1561.5 1502.5]\n",
            "[0.04315805 0.04235823 0.04352039 0.8709633 ]\n",
            "The average reward for each of the 3 bandits: [1693.  1638.5 1576. ]\n",
            "[0.04022023 0.8835416  0.03701367 0.03922449]\n",
            "The average reward for each of the 3 bandits: [1781.5 1702.5 1642.5]\n",
            "[0.90638345 0.03181692 0.03083435 0.03096519]\n",
            "The average reward for each of the 3 bandits: [1862.5 1781.  1711. ]\n",
            "[0.03668022 0.03609268 0.03697087 0.8902563 ]\n",
            "The average reward for each of the 3 bandits: [1935.5 1856.5 1777.5]\n",
            "[0.03466995 0.89994377 0.03155395 0.03383235]\n",
            "The average reward for each of the 3 bandits: [2002.5 1938.  1851. ]\n",
            "[0.03341163 0.90385365 0.03031601 0.03241876]\n",
            "The average reward for each of the 3 bandits: [2083.  2012.5 1926. ]\n",
            "[0.03197772 0.90744954 0.02910073 0.03147202]\n",
            "The average reward for each of the 3 bandits: [2154.5 2091.  2002. ]\n",
            "[0.9307731  0.02335753 0.02284596 0.02302341]\n",
            "The average reward for each of the 3 bandits: [2231.5 2174.  2073. ]\n",
            "[0.02992919 0.02976694 0.03075379 0.90955013]\n",
            "The average reward for each of the 3 bandits: [2308.5 2248.  2143. ]\n",
            "[0.02932696 0.02922771 0.03000769 0.9114377 ]\n",
            "The average reward for each of the 3 bandits: [2385.  2322.5 2215. ]\n",
            "[0.9401296  0.02031539 0.01980028 0.01975477]\n",
            "The average reward for each of the 3 bandits: [2458.  2395.  2292.5]\n",
            "[0.02744736 0.9219278  0.0240657  0.0265592 ]\n",
            "The average reward for each of the 3 bandits: [2530.  2485.  2354.5]\n",
            "[0.0267267  0.9244721  0.02308659 0.02571457]\n",
            "The average reward for each of the 3 bandits: [2608.5 2560.5 2424.5]\n",
            "[0.02556787 0.02533467 0.02578986 0.9233076 ]\n",
            "The average reward for each of the 3 bandits: [2683.5 2633.5 2503.5]\n",
            "[0.02466538 0.930714   0.02097098 0.02364974]\n",
            "The average reward for each of the 3 bandits: [2763.5 2706.  2579. ]\n",
            "[0.02405514 0.02398171 0.0240679  0.92789525]\n",
            "The average reward for each of the 3 bandits: [2843.5 2787.  2642. ]\n",
            "[0.02332351 0.93479687 0.0196794  0.02220016]\n",
            "The average reward for each of the 3 bandits: [2917.  2866.  2711.5]\n",
            "[0.9571328  0.01458364 0.01403244 0.01425098]\n",
            "The average reward for each of the 3 bandits: [3004.5 2939.  2774. ]\n",
            "The most likely action is 4 probs: [0.06947555 0.07053506 0.06802794 3.7919567 ] for bandit 1\n",
            ":) Yes, its correct!\n",
            "The most likely action is 2 probs: [ 0.10852018  3.8828676  -0.05524991  0.06385963] for bandit 2\n",
            ":) Yes, its correct!\n",
            "The most likely action is 1 probs: [ 4.192956   -0.04651843 -0.0829084  -0.06352255] for bandit 3\n",
            ":) Yes, its correct!\n"
          ]
        }
      ],
      "source": [
        "#tf.reset_default_graph()  \n",
        "tf.compat.v1.reset_default_graph()\n",
        "#Loading the contextual bandit.\n",
        "cBandit = contextual_bandit()\n",
        "#Loading the agent.\n",
        "myAgent = agent(lr=0.001, s_size=cBandit.num_bandits, a_size=cBandit.num_actions)\n",
        "#Lets define the weights which are optimizer during the training phase for making maximum rewards\n",
        "weights = tf.trainable_variables()[0]\n",
        "\n",
        "#The total number of games played by the agent.\n",
        "total_episodes = 20000 \n",
        "#Set the scoreboard to zero.\n",
        "total_rewards = np.zeros([cBandit.num_bandits, cBandit.num_actions])\n",
        "#The chance of taking a random action.\n",
        "e = 0.1\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    i = 0\n",
        "    while(i<total_episodes):\n",
        "        #Get a random state from the environment.\n",
        "        s = cBandit.getBandit()\n",
        "        \n",
        "        #Choose either a random action or one from our network.\n",
        "        if(np.random.rand(1) < e):\n",
        "            action = np.random.randint(cBandit.num_actions)\n",
        "        else:\n",
        "            action = sess.run(myAgent.choosen_action, feed_dict={myAgent.state_in: [s]})\n",
        "            action_prob = sess.run(myAgent.output, feed_dict={myAgent.state_in: [s]})\n",
        "        \n",
        "        #Now lets perform the action(pull arm) to observe the reward. \n",
        "        reward = cBandit.pullArm(action)\n",
        "        \n",
        "        #Update the network\n",
        "        feed_dict = {myAgent.reward_holder: [reward], myAgent.action_holder: [action], myAgent.state_in: [s]}\n",
        "        _, ww = sess.run([myAgent.update, weights], feed_dict=feed_dict)\n",
        "        #import pdb; pdb.set_trace()\n",
        "        #Update our scoreboard\n",
        "        total_rewards[s, action] += reward\n",
        "        if(i%500==0):\n",
        "            print(action_prob)\n",
        "            print(\"The average reward for each of the \" + str(cBandit.num_bandits) + \" bandits: \" + str(np.mean(total_rewards, axis=1)))\n",
        "        i += 1\n",
        " \n",
        "#Let's evaluate our actions.\n",
        "for a in range(cBandit.num_bandits):\n",
        "    print(\"The most likely action is \" + str(np.argmax(ww[a])+1) + \" probs: \" + str(ww[a]) + \" for bandit \" + str(a+1))\n",
        "    if(np.argmax(ww[a]) == np.argmin(cBandit.bandits[a])):\n",
        "        print(\":) Yes, its correct!\")\n",
        "    else:\n",
        "        print(\":( No, it isn't!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_vqpeMJSG1PP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}